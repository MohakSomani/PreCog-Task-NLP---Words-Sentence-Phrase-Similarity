{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohakSomani/PreCog-Task-NLP---Words-Sentence-Phrase-Similarity/blob/main/task_a_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sMnv2AlV5k1K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# print(torch.cuda.is_available())\n",
        "# print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2dSTTg-o5k1L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "80ifne1U5k1L",
        "outputId": "2dd0d5fd-a279-4a60-b551-018183fc2333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download the Brown corpus if not already downloaded\n",
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bAd8tPaA5k1M"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "sentences = brown.sents()[:]  # Use only the first 1000 sentences for quick testing\n",
        "corpus = [word.lower() for sentence in sentences for word in sentence]\n",
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6tRzbHCP5k1M"
      },
      "outputs": [],
      "source": [
        "# Create word to index and index to word mappings\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O-z6IxmF5k1M"
      },
      "outputs": [],
      "source": [
        "# Generate training data (CBOW model)\n",
        "def generate_cbow_data(corpus, window_size=2):\n",
        "    data = []\n",
        "    for idx, word in enumerate(corpus):\n",
        "        if idx < window_size or idx >= len(corpus) - window_size:\n",
        "            continue\n",
        "        context = [word_to_index[corpus[i]] for i in range(idx - window_size, idx + window_size + 1) if i != idx]\n",
        "        target = word_to_index[word]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "window_size = 2\n",
        "training_data = generate_cbow_data(corpus, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2COcWNNo5k1M"
      },
      "outputs": [],
      "source": [
        "# Define the Word2Vec model\n",
        "class Word2VecCBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2VecCBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embedded = self.embeddings(context).mean(dim=1)\n",
        "        output = self.output(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iIP-LUiF5k1N"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 50  # Reduced for faster training\n",
        "epochs = 10         # Fewer epochs for quick testing\n",
        "batch_size = 64    # Smaller batch size for quicker iterations\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mflf8P7f5k1N",
        "outputId": "fc60055c-8173-4831-d0c8-5280868a8a0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cyxnDFov5k1N"
      },
      "outputs": [],
      "source": [
        "# Create model, loss function, and optimizer\n",
        "model = Word2VecCBOW(vocab_size, embedding_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NksASdUq5k1N"
      },
      "outputs": [],
      "source": [
        "# Prepare batches for training\n",
        "def get_batches(training_data, batch_size):\n",
        "    random.shuffle(training_data)\n",
        "    for i in range(0, len(training_data), batch_size):\n",
        "        batch = training_data[i:i + batch_size]\n",
        "        contexts, targets = zip(*batch)\n",
        "        yield torch.tensor(contexts, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-P570Qr-5k1O",
        "outputId": "d1471c72-46b1-4715-f7bf-8f8a48ccbbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 121068.8386\n",
            "Epoch 2, Loss: 109024.3246\n",
            "Epoch 3, Loss: 104458.1656\n",
            "Epoch 4, Loss: 101682.8668\n",
            "Epoch 5, Loss: 99866.2830\n",
            "Epoch 6, Loss: 98606.1743\n",
            "Epoch 7, Loss: 97735.5712\n",
            "Epoch 8, Loss: 97029.2780\n",
            "Epoch 9, Loss: 96555.7973\n",
            "Epoch 10, Loss: 96208.1430\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, targets in get_batches(training_data, batch_size):\n",
        "        contexts, targets = contexts.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t-ZrtcQU5k1O"
      },
      "outputs": [],
      "source": [
        "# Save the word embeddings\n",
        "word_embeddings = model.embeddings.weight.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yDwIBtvU5k1O"
      },
      "outputs": [],
      "source": [
        "# # Example usage: find similar words\n",
        "# def find_similar_words(word, word_embeddings, top_n=5):\n",
        "#     if word not in word_to_index:\n",
        "#         return []\n",
        "#     idx = word_to_index[word]\n",
        "#     word_vec = word_embeddings[idx]\n",
        "#     similarities = np.dot(word_embeddings, word_vec) / (\n",
        "#         np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vec))\n",
        "#     similar_indices = similarities.argsort()[-top_n - 1:-1][::-1]\n",
        "#     return [index_to_word[i] for i in similar_indices if i != idx]\n",
        "\n",
        "# print(find_similar_words(\"government\", word_embeddings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IzCW_Mny5k1O",
        "outputId": "8dd15f62-9b59-4739-b259-5fb392fbc493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'king' and 'queen': 0.3180\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_similarity_score(word1, word2, word_embeddings, word_to_index):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity score between two words using word embeddings.\n",
        "\n",
        "    Args:\n",
        "        word1 (str): The first word.\n",
        "        word2 (str): The second word.\n",
        "        word_embeddings (np.ndarray): The word embeddings matrix.\n",
        "        word_to_index (dict): A dictionary mapping words to their indices in the embeddings matrix.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity score between the two words.\n",
        "    \"\"\"\n",
        "    if word1 not in word_to_index or word2 not in word_to_index:\n",
        "        return 0.0  # Return 0 if either word is not in the vocabulary\n",
        "\n",
        "    word1_index = word_to_index[word1]\n",
        "    word2_index = word_to_index[word2]\n",
        "\n",
        "    word1_vec = word_embeddings[word1_index]\n",
        "    word2_vec = word_embeddings[word2_index]\n",
        "\n",
        "    similarity = np.dot(word1_vec, word2_vec) / (np.linalg.norm(word1_vec) * np.linalg.norm(word2_vec))\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "similarity_score = get_similarity_score(word1, word2, word_embeddings, word_to_index)\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}