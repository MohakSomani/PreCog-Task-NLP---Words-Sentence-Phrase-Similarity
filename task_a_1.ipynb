{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sMnv2AlV5k1K",
        "outputId": "28d44333-2b31-43de-a3de-e936712045e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# print(torch.cuda.is_available())  # Should return True\n",
        "# print(torch.cuda.get_device_name(0))  # Should print NVIDIA RTX 4060\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2dSTTg-o5k1L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "80ifne1U5k1L",
        "outputId": "218652f3-0711-4772-f0ca-0bd05a8c6ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Download the Brown corpus if not already downloaded\n",
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bAd8tPaA5k1M"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "sentences = brown.sents()[:]  # Use only the first 1000 sentences for quick testing\n",
        "corpus = [word.lower() for sentence in sentences for word in sentence]\n",
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6tRzbHCP5k1M"
      },
      "outputs": [],
      "source": [
        "# Create word to index and index to word mappings\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O-z6IxmF5k1M"
      },
      "outputs": [],
      "source": [
        "# Generate training data (CBOW model)\n",
        "def generate_cbow_data(corpus, window_size=2):\n",
        "    data = []\n",
        "    for idx, word in enumerate(corpus):\n",
        "        if idx < window_size or idx >= len(corpus) - window_size:\n",
        "            continue\n",
        "        context = [word_to_index[corpus[i]] for i in range(idx - window_size, idx + window_size + 1) if i != idx]\n",
        "        target = word_to_index[word]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "window_size = 2\n",
        "training_data = generate_cbow_data(corpus, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2COcWNNo5k1M"
      },
      "outputs": [],
      "source": [
        "# Define the Word2Vec model\n",
        "class Word2VecCBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2VecCBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embedded = self.embeddings(context).mean(dim=1)\n",
        "        output = self.output(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iIP-LUiF5k1N"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 50  # Reduced for faster training\n",
        "epochs = 10         # Fewer epochs for quick testing\n",
        "batch_size = 64    # Smaller batch size for quicker iterations\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Mflf8P7f5k1N",
        "outputId": "c62f0733-c321-46eb-cd5d-3978f15c154e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cyxnDFov5k1N"
      },
      "outputs": [],
      "source": [
        "# Create model, loss function, and optimizer\n",
        "model = Word2VecCBOW(vocab_size, embedding_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NksASdUq5k1N"
      },
      "outputs": [],
      "source": [
        "# Prepare batches for training\n",
        "def get_batches(training_data, batch_size):\n",
        "    random.shuffle(training_data)\n",
        "    for i in range(0, len(training_data), batch_size):\n",
        "        batch = training_data[i:i + batch_size]\n",
        "        contexts, targets = zip(*batch)\n",
        "        yield torch.tensor(contexts, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-P570Qr-5k1O",
        "outputId": "602a30ed-b613-45e6-c0a1-06a1368fcb05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 121014.6922\n",
            "Epoch 2, Loss: 108997.5691\n",
            "Epoch 3, Loss: 104398.8696\n",
            "Epoch 4, Loss: 101637.1534\n",
            "Epoch 5, Loss: 99831.4014\n",
            "Epoch 6, Loss: 98582.6999\n",
            "Epoch 7, Loss: 97716.0739\n",
            "Epoch 8, Loss: 97060.1915\n",
            "Epoch 9, Loss: 96601.6101\n",
            "Epoch 10, Loss: 96197.4693\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, targets in get_batches(training_data, batch_size):\n",
        "        contexts, targets = contexts.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t-ZrtcQU5k1O"
      },
      "outputs": [],
      "source": [
        "# Save the word embeddings\n",
        "word_embeddings = model.embeddings.weight.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yDwIBtvU5k1O",
        "outputId": "21b185aa-170f-440d-9d2a-87c8c8a254b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['intervention', 'electronics', 'nominee', 'elections', 'outskirts']\n"
          ]
        }
      ],
      "source": [
        "# # Example usage: find similar words\n",
        "# def find_similar_words(word, word_embeddings, top_n=5):\n",
        "#     if word not in word_to_index:\n",
        "#         return []\n",
        "#     idx = word_to_index[word]\n",
        "#     word_vec = word_embeddings[idx]\n",
        "#     similarities = np.dot(word_embeddings, word_vec) / (\n",
        "#         np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vec))\n",
        "#     similar_indices = similarities.argsort()[-top_n - 1:-1][::-1]\n",
        "#     return [index_to_word[i] for i in similar_indices if i != idx]\n",
        "\n",
        "# print(find_similar_words(\"government\", word_embeddings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IzCW_Mny5k1O",
        "outputId": "8e083418-b30f-4437-a06e-60f2147ab900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'government' and 'politics': 0.3350\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_similarity_score(word1, word2, word_embeddings, word_to_index):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity score between two words using word embeddings.\n",
        "\n",
        "    Args:\n",
        "        word1 (str): The first word.\n",
        "        word2 (str): The second word.\n",
        "        word_embeddings (np.ndarray): The word embeddings matrix.\n",
        "        word_to_index (dict): A dictionary mapping words to their indices in the embeddings matrix.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity score between the two words.\n",
        "    \"\"\"\n",
        "    if word1 not in word_to_index or word2 not in word_to_index:\n",
        "        return 0.0  # Return 0 if either word is not in the vocabulary\n",
        "\n",
        "    word1_index = word_to_index[word1]\n",
        "    word2_index = word_to_index[word2]\n",
        "\n",
        "    word1_vec = word_embeddings[word1_index]\n",
        "    word2_vec = word_embeddings[word2_index]\n",
        "\n",
        "    similarity = np.dot(word1_vec, word2_vec) / (np.linalg.norm(word1_vec) * np.linalg.norm(word2_vec))\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "word1 = \"government\"\n",
        "word2 = \"politics\"\n",
        "similarity_score = get_similarity_score(word1, word2, word_embeddings, word_to_index)\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}