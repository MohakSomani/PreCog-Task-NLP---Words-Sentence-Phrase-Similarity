{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohakSomani/PreCog-Task-NLP---Words-Sentence-Phrase-Similarity/blob/main/task_a_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sMnv2AlV5k1K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# print(torch.cuda.is_available())\n",
        "# print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2dSTTg-o5k1L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "80ifne1U5k1L",
        "outputId": "db938493-1fa9-477c-af30-76d3c6a8d867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Download the Brown corpus if not already downloaded\n",
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bAd8tPaA5k1M"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "sentences = brown.sents()[:]  # Use only the first 1000 sentences for quick testing\n",
        "corpus = [word.lower() for sentence in sentences for word in sentence]\n",
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6tRzbHCP5k1M"
      },
      "outputs": [],
      "source": [
        "# Create word to index and index to word mappings\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O-z6IxmF5k1M"
      },
      "outputs": [],
      "source": [
        "# Generate training data (CBOW model)\n",
        "def generate_cbow_data(corpus, window_size=2):\n",
        "    data = []\n",
        "    for idx, word in enumerate(corpus):\n",
        "        if idx < window_size or idx >= len(corpus) - window_size:\n",
        "            continue\n",
        "        context = [word_to_index[corpus[i]] for i in range(idx - window_size, idx + window_size + 1) if i != idx]\n",
        "        target = word_to_index[word]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "window_size = 2\n",
        "training_data = generate_cbow_data(corpus, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2COcWNNo5k1M"
      },
      "outputs": [],
      "source": [
        "# Define the Word2Vec model\n",
        "class Word2VecCBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2VecCBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embedded = self.embeddings(context).mean(dim=1)\n",
        "        output = self.output(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iIP-LUiF5k1N"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 50\n",
        "epochs = 1\n",
        "batch_size = 64\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Mflf8P7f5k1N",
        "outputId": "fe3a92c1-f549-4ad9-9c98-6b5ac240d759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cyxnDFov5k1N"
      },
      "outputs": [],
      "source": [
        "# Create model, loss function, and optimizer\n",
        "model = Word2VecCBOW(vocab_size, embedding_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NksASdUq5k1N"
      },
      "outputs": [],
      "source": [
        "# Prepare batches for training\n",
        "def get_batches(training_data, batch_size):\n",
        "    random.shuffle(training_data)\n",
        "    for i in range(0, len(training_data), batch_size):\n",
        "        batch = training_data[i:i + batch_size]\n",
        "        contexts, targets = zip(*batch)\n",
        "        yield torch.tensor(contexts, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-P570Qr-5k1O",
        "outputId": "bc54ef36-02f6-4726-a7d1-6482b620a7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c55bafba3e8f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, targets in get_batches(training_data, batch_size):\n",
        "        contexts, targets = contexts.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "t-ZrtcQU5k1O"
      },
      "outputs": [],
      "source": [
        "# Save the word embeddings\n",
        "word_embeddings = model.embeddings.weight.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yDwIBtvU5k1O"
      },
      "outputs": [],
      "source": [
        "# # Example usage: find similar words\n",
        "# def find_similar_words(word, word_embeddings, top_n=5):\n",
        "#     if word not in word_to_index:\n",
        "#         return []\n",
        "#     idx = word_to_index[word]\n",
        "#     word_vec = word_embeddings[idx]\n",
        "#     similarities = np.dot(word_embeddings, word_vec) / (\n",
        "#         np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(word_vec))\n",
        "#     similar_indices = similarities.argsort()[-top_n - 1:-1][::-1]\n",
        "#     return [index_to_word[i] for i in similar_indices if i != idx]\n",
        "\n",
        "# print(find_similar_words(\"government\", word_embeddings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IzCW_Mny5k1O",
        "outputId": "a7d1ee07-47c5-4984-a6d6-3bf740d9b493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'king' and 'queen': -0.0835\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_similarity_score(word1, word2, word_embeddings, word_to_index):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity score between two words using word embeddings.\n",
        "\n",
        "    Args:\n",
        "        word1 (str): The first word.\n",
        "        word2 (str): The second word.\n",
        "        word_embeddings (np.ndarray): The word embeddings matrix.\n",
        "        word_to_index (dict): A dictionary mapping words to their indices in the embeddings matrix.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity score between the two words.\n",
        "    \"\"\"\n",
        "    if word1 not in word_to_index or word2 not in word_to_index:\n",
        "        return 0.0  # Return 0 if either word is not in the vocabulary\n",
        "\n",
        "    word1_index = word_to_index[word1]\n",
        "    word2_index = word_to_index[word2]\n",
        "\n",
        "    word1_vec = word_embeddings[word1_index]\n",
        "    word2_vec = word_embeddings[word2_index]\n",
        "\n",
        "    #Cosine Similarity of two embeddings\n",
        "    similarity = np.dot(word1_vec, word2_vec) / (np.linalg.norm(word1_vec) * np.linalg.norm(word2_vec))\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "similarity_score = get_similarity_score(word1, word2, word_embeddings, word_to_index)\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the test dataset from a tab-separated text file\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "def load_txt_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Loads a tab-separated text file as a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the tab-separated dataset file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the dataset.\n",
        "    \"\"\"\n",
        "    columns = ['word1', 'word2', 'POS', 'SimLex999', 'conc(w1)', 'conc(w2)',\n",
        "               'concQ', 'Assoc(USF)', 'SimAssoc333', 'SD(SimLex)']\n",
        "    return pd.read_csv(file_path, delimiter=\"\\t\", names=columns, skiprows=1)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model_with_txt(file_path, word_embeddings, word_to_index):\n",
        "    \"\"\"\n",
        "    Evaluates the model using a tab-separated test dataset with human-rated similarity scores.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the test dataset file.\n",
        "        word_embeddings (np.ndarray): The word embeddings matrix.\n",
        "        word_to_index (dict): A dictionary mapping words to their indices in the embeddings matrix.\n",
        "\n",
        "    Returns:\n",
        "        dict: Pearson and Spearman correlation scores.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    test_data = load_txt_dataset(file_path)\n",
        "\n",
        "    # Extract word pairs and human similarity scores\n",
        "    word_pairs = test_data[['word1', 'word2']]\n",
        "    human_scores = test_data['SimLex999']\n",
        "\n",
        "    # Compute model-predicted similarity scores\n",
        "    predicted_scores = []\n",
        "    for _, row in word_pairs.iterrows():\n",
        "        similarity = get_similarity_score(row['word1'], row['word2'], word_embeddings, word_to_index)\n",
        "        predicted_scores.append(similarity)\n",
        "\n",
        "    # Filter out missing words (None scores)\n",
        "    valid_indices = [i for i, score in enumerate(predicted_scores) if score is not None]\n",
        "    human_scores = human_scores.iloc[valid_indices]\n",
        "    predicted_scores = np.array([predicted_scores[i] for i in valid_indices])\n",
        "\n",
        "    # Calculate Pearson and Spearman correlations\n",
        "    pearson_corr, _ = pearsonr(human_scores, predicted_scores)\n",
        "    spearman_corr, _ = spearmanr(human_scores, predicted_scores)\n",
        "\n",
        "    return {\n",
        "        \"Pearson Correlation\": pearson_corr,\n",
        "        \"Spearman Correlation\": spearman_corr\n",
        "    }\n",
        "\n",
        "# Example usage (Jupyter Notebook cell)\n",
        "# Path to your dataset file\n",
        "test_dataset_path = \"SimLex-999.txt\"\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_results = evaluate_model_with_txt(test_dataset_path, word_embeddings, word_to_index)\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Pearson Correlation: {evaluation_results['Pearson Correlation']:.4f}\")\n",
        "print(f\"Spearman Correlation: {evaluation_results['Spearman Correlation']:.4f}\")\n"
      ],
      "metadata": {
        "id": "B2EzeTVYLPZF",
        "outputId": "295ddd7a-326c-4087-f3e5-515662c9ba3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "Pearson Correlation: -0.0164\n",
            "Spearman Correlation: -0.0175\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}